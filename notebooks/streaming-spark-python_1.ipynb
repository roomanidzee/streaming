{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e55a5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e88613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\r\n",
      "----------------------------- ---------\r\n",
      "alembic                       1.7.4\r\n",
      "altair                        4.1.0\r\n",
      "anyio                         3.3.2\r\n",
      "appdirs                       1.4.4\r\n",
      "argon2-cffi                   20.1.0\r\n",
      "async-generator               1.10\r\n",
      "attrs                         21.2.0\r\n",
      "Babel                         2.9.1\r\n",
      "backcall                      0.2.0\r\n",
      "backports.functools-lru-cache 1.6.4\r\n",
      "beautifulsoup4                4.10.0\r\n",
      "bleach                        4.1.0\r\n",
      "blinker                       1.4\r\n",
      "bokeh                         2.4.0\r\n",
      "Bottleneck                    1.3.2\r\n",
      "brotlipy                      0.7.0\r\n",
      "cached-property               1.5.2\r\n",
      "certifi                       2021.5.30\r\n",
      "certipy                       0.1.3\r\n",
      "cffi                          1.14.6\r\n",
      "chardet                       4.0.0\r\n",
      "charset-normalizer            2.0.0\r\n",
      "click                         8.0.1\r\n",
      "cloudpickle                   2.0.0\r\n",
      "colorama                      0.4.4\r\n",
      "conda                         4.10.3\r\n",
      "conda-package-handling        1.7.3\r\n",
      "cryptography                  3.4.7\r\n",
      "cycler                        0.10.0\r\n",
      "Cython                        0.29.24\r\n",
      "cytoolz                       0.11.0\r\n",
      "dask                          2021.9.1\r\n",
      "debugpy                       1.4.1\r\n",
      "decorator                     5.1.0\r\n",
      "defusedxml                    0.7.1\r\n",
      "dill                          0.3.4\r\n",
      "distributed                   2021.9.1\r\n",
      "entrypoints                   0.3\r\n",
      "fastavro                      1.4.5\r\n",
      "findspark                     1.3.0\r\n",
      "fsspec                        2021.10.0\r\n",
      "gmpy2                         2.1.0b5\r\n",
      "greenlet                      1.1.2\r\n",
      "h5py                          3.4.0\r\n",
      "HeapDict                      1.0.1\r\n",
      "idna                          3.1\r\n",
      "imagecodecs                   2021.7.30\r\n",
      "imageio                       2.9.0\r\n",
      "importlib-metadata            4.8.1\r\n",
      "importlib-resources           5.2.2\r\n",
      "ipykernel                     6.4.1\r\n",
      "ipympl                        0.8.0\r\n",
      "ipyparallel                   7.1.0\r\n",
      "ipython                       7.28.0\r\n",
      "ipython-genutils              0.2.0\r\n",
      "ipywidgets                    7.6.5\r\n",
      "jedi                          0.18.0\r\n",
      "Jinja2                        3.0.1\r\n",
      "joblib                        1.0.1\r\n",
      "json5                         0.9.5\r\n",
      "jsonschema                    4.0.1\r\n",
      "jupyter-client                7.0.6\r\n",
      "jupyter-core                  4.8.1\r\n",
      "jupyter-server                1.11.1\r\n",
      "jupyter-telemetry             0.1.0\r\n",
      "jupyterhub                    1.4.2\r\n",
      "jupyterlab                    3.1.17\r\n",
      "jupyterlab-pygments           0.1.2\r\n",
      "jupyterlab-server             2.8.2\r\n",
      "jupyterlab-widgets            1.0.2\r\n",
      "kiwisolver                    1.3.2\r\n",
      "llvmlite                      0.37.0\r\n",
      "locket                        0.2.0\r\n",
      "Mako                          1.1.5\r\n",
      "mamba                         0.16.0\r\n",
      "MarkupSafe                    2.0.1\r\n",
      "matplotlib                    3.4.3\r\n",
      "matplotlib-inline             0.1.3\r\n",
      "metakernel                    0.27.5\r\n",
      "mistune                       0.8.4\r\n",
      "mock                          4.0.3\r\n",
      "mpmath                        1.2.1\r\n",
      "msgpack                       1.0.2\r\n",
      "nbclassic                     0.3.2\r\n",
      "nbclient                      0.5.4\r\n",
      "nbconvert                     6.2.0\r\n",
      "nbformat                      5.1.3\r\n",
      "nest-asyncio                  1.5.1\r\n",
      "networkx                      2.5\r\n",
      "notebook                      6.4.4\r\n",
      "numba                         0.54.0\r\n",
      "numexpr                       2.7.3\r\n",
      "numpy                         1.20.3\r\n",
      "oauthlib                      3.1.1\r\n",
      "olefile                       0.46\r\n",
      "packaging                     21.0\r\n",
      "pamela                        1.0.0\r\n",
      "pandas                        1.3.3\r\n",
      "pandocfilters                 1.5.0\r\n",
      "parso                         0.8.2\r\n",
      "partd                         1.2.0\r\n",
      "patsy                         0.5.2\r\n",
      "pexpect                       4.8.0\r\n",
      "pickleshare                   0.7.5\r\n",
      "Pillow                        8.3.2\r\n",
      "pip                           21.2.4\r\n",
      "pooch                         1.5.1\r\n",
      "portalocker                   2.3.2\r\n",
      "prometheus-client             0.11.0\r\n",
      "prompt-toolkit                3.0.20\r\n",
      "protobuf                      3.18.1\r\n",
      "psutil                        5.8.0\r\n",
      "ptyprocess                    0.7.0\r\n",
      "pyarrow                       5.0.0\r\n",
      "pycosat                       0.6.3\r\n",
      "pycparser                     2.20\r\n",
      "pycurl                        7.44.1\r\n",
      "Pygments                      2.10.0\r\n",
      "PyJWT                         2.1.0\r\n",
      "pyOpenSSL                     21.0.0\r\n",
      "pyparsing                     2.4.7\r\n",
      "pyrsistent                    0.17.3\r\n",
      "PySocks                       1.7.1\r\n",
      "pyspark                       3.1.2\r\n",
      "python-dateutil               2.8.2\r\n",
      "python-json-logger            2.0.1\r\n",
      "pytz                          2021.3\r\n",
      "PyWavelets                    1.1.1\r\n",
      "PyYAML                        5.4.1\r\n",
      "pyzmq                         22.3.0\r\n",
      "requests                      2.26.0\r\n",
      "requests-unixsocket           0.2.0\r\n",
      "ruamel.yaml                   0.17.16\r\n",
      "ruamel.yaml.clib              0.2.2\r\n",
      "ruamel-yaml-conda             0.15.80\r\n",
      "scikit-image                  0.18.3\r\n",
      "scikit-learn                  1.0\r\n",
      "scipy                         1.7.1\r\n",
      "seaborn                       0.11.2\r\n",
      "Send2Trash                    1.8.0\r\n",
      "setuptools                    58.2.0\r\n",
      "six                           1.16.0\r\n",
      "sniffio                       1.2.0\r\n",
      "sortedcontainers              2.4.0\r\n",
      "soupsieve                     2.0.1\r\n",
      "spylon                        0.3.0\r\n",
      "spylon-kernel                 0.4.1\r\n",
      "SQLAlchemy                    1.4.25\r\n",
      "statsmodels                   0.13.0\r\n",
      "sympy                         1.8\r\n",
      "tables                        3.6.1\r\n",
      "tblib                         1.7.0\r\n",
      "terminado                     0.12.1\r\n",
      "testpath                      0.5.0\r\n",
      "threadpoolctl                 3.0.0\r\n",
      "tifffile                      2021.8.30\r\n",
      "toolz                         0.11.1\r\n",
      "tornado                       6.1\r\n",
      "tqdm                          4.62.3\r\n",
      "traitlets                     5.1.0\r\n",
      "typing-extensions             3.10.0.2\r\n",
      "urllib3                       1.26.7\r\n",
      "wcwidth                       0.2.5\r\n",
      "webencodings                  0.5.1\r\n",
      "websocket-client              0.57.0\r\n",
      "wheel                         0.37.0\r\n",
      "widgetsnbextension            3.5.1\r\n",
      "xlrd                          2.0.1\r\n",
      "zict                          2.0.0\r\n",
      "zipp                          3.6.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0511392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastavro in /opt/conda/lib/python3.9/site-packages (1.4.5)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9303b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c685c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ee65c147-b5a9-408d-88c8-e543863791be;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 599ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ee65c147-b5a9-408d-88c8-e543863791be\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/15ms)\n",
      "21/10/09 15:01:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('person-data-analyzer')\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\n",
    "         .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a9d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9302e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-server:9092\")\n",
    "  .option(\"subscribe\", \"persons-json\")\n",
    "  .option(\"startingOffsets\", \"earliest\") \n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f025bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68b9e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be79ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf2ca5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06b3dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_json_schema = StructType(\n",
    "  [\n",
    "      StructField(\"id\", LongType(), True),\n",
    "      StructField(\"firstname\", StringType(), True),\n",
    "      StructField(\"lastname\", StringType(), True),\n",
    "      StructField(\"email\", StringType(), True),\n",
    "      StructField(\"profession\", StringType(), True),\n",
    "      StructField(\"city\", StringType(), True),\n",
    "      StructField(\"country\", StringType(), True),\n",
    "      StructField(\"random_date\", TimestampType(), True)\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0e6133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df_json = df1.withColumn(\"schema_value\", from_json(\"value\", person_json_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be600b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+---------+------+--------------------+-------------+--------------------+\n",
      "| key|               value|       topic|partition|offset|           timestamp|timestampType|        schema_value|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+--------------------+\n",
      "|null|{\"id\": 1, \"firstn...|persons-json|        0|     0|2021-10-09 14:22:...|            0|{1, Cyndie, Chain...|\n",
      "|null|{\"id\": 2, \"firstn...|persons-json|        0|     1|2021-10-09 14:22:...|            0|{2, Edyth, Bultma...|\n",
      "|null|{\"id\": 3, \"firstn...|persons-json|        0|     2| 2021-10-09 14:22:45|            0|{3, Mignon, Martg...|\n",
      "|null|{\"id\": 4, \"firstn...|persons-json|        0|     3|2021-10-09 14:22:...|            0|{4, Allis, Aberno...|\n",
      "|null|{\"id\": 5, \"firstn...|persons-json|        0|     4|2021-10-09 14:22:...|            0|{5, Morganica, Qu...|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df_json.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b00b171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "person_df_json_formatted = (\n",
    "   person_df_json.select(\n",
    "     col(\"timestamp\").alias(\"event_timestamp\"),\n",
    "     col(\"schema_value.id\").alias(\"id\"),\n",
    "     col(\"schema_value.firstname\").alias(\"first_name\"),\n",
    "     col(\"schema_value.lastname\").alias(\"last_name\"),\n",
    "     col(\"schema_value.email\").alias(\"email\"),\n",
    "     col(\"schema_value.profession\").alias(\"profession\"),\n",
    "     col(\"schema_value.city\").alias(\"city\"),\n",
    "     col(\"schema_value.country\").alias(\"country\"),\n",
    "     col(\"schema_value.random_date\").alias(\"random_date\")  \n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5190464a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----------+-----------+--------------------+-----------+----------+--------------------+-----------+\n",
      "|     event_timestamp| id|first_name|  last_name|               email| profession|      city|             country|random_date|\n",
      "+--------------------+---+----------+-----------+--------------------+-----------+----------+--------------------+-----------+\n",
      "|2021-10-09 14:22:...|  1|    Cyndie|     Chaing|Cyndie.Chaing@yop...|     worker|     Lhasa|              Mexico|       null|\n",
      "|2021-10-09 14:22:...|  2|     Edyth|    Bultman|Edyth.Bultman@yop...|firefighter|   Cologne|         New Zealand|       null|\n",
      "| 2021-10-09 14:22:45|  3|    Mignon|Martguerita|Mignon.Martguerit...|     worker|Concepción|       Bouvet Island|       null|\n",
      "|2021-10-09 14:22:...|  4|     Allis|    Abernon|Allis.Abernon@yop...|  developer|   Geelong|         Isle of Man|       null|\n",
      "|2021-10-09 14:22:...|  5| Morganica|      Quent|Morganica.Quent@y...|     worker|   Pattaya|Saint Pierre and ...|       null|\n",
      "+--------------------+---+----------+-----------+--------------------+-----------+----------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_df_json_formatted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b4d1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "person_df_json_counted = (\n",
    "  person_df_json_formatted.groupby(col(\"profession\"))\n",
    "                          .agg(count(col(\"profession\")).alias(\"profession_count\")) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ab0b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|    profession|profession_count|\n",
      "+--------------+----------------+\n",
      "|     developer|           19878|\n",
      "|   firefighter|           20048|\n",
      "|police officer|           19905|\n",
      "|        worker|           20154|\n",
      "|        doctor|           20015|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:===========================================>            (58 + 1) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "person_df_json_counted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eba1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd98190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_avro(serialized_msg):\n",
    "    bytes_io = io.BytesIO(serialized_msg)\n",
    "    bytes_io.seek(0)\n",
    "    avro_schema = {\n",
    "        'type': 'record',\n",
    "        'name': 'PersonRecord',\n",
    "        'fields': [\n",
    "            {'name': 'id', 'type': 'long'},\n",
    "            {'name': 'firstname', 'type': 'string'},\n",
    "            {'name': 'lastname', 'type': 'string'},\n",
    "            {'name': 'email', 'type': 'string'},\n",
    "            {'name': 'profession', 'type': 'string'},\n",
    "            {'name': 'city', 'type': 'string'},\n",
    "            {'name': 'country', 'type': 'string'},\n",
    "            {'name': 'random_date', 'type': {'type': 'long', 'logicalType': 'timestamp-millis'}}\n",
    "        ],\n",
    "        'doc': 'Definition of Person record model',\n",
    "        'namespace': 'Person.v1',\n",
    "        'aliases': ['person-v1', 'person-record']\n",
    "    }\n",
    "\n",
    "    deserialized_msg = fastavro.schemaless_reader(bytes_io, avro_schema)\n",
    "\n",
    "    return (    deserialized_msg[\"id\"],\n",
    "                deserialized_msg[\"firstname\"],\n",
    "                deserialized_msg[\"lastname\"],\n",
    "                deserialized_msg[\"email\"],\n",
    "                deserialized_msg[\"profession\"],\n",
    "                deserialized_msg[\"city\"],\n",
    "                deserialized_msg[\"country\"],\n",
    "                deserialized_msg[\"random_date\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f48c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-server:9092\")\n",
    "  .option(\"subscribe\", \"persons-avro\")\n",
    "  .option(\"startingOffsets\", \"earliest\") \n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca42a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "813fa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4586479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_avro_schema = StructType(\n",
    "  [\n",
    "      StructField(\"id\", LongType(), True),\n",
    "      StructField(\"firstname\", StringType(), True),\n",
    "      StructField(\"lastname\", StringType(), True),\n",
    "      StructField(\"email\", StringType(), True),\n",
    "      StructField(\"profession\", StringType(), True),\n",
    "      StructField(\"city\", StringType(), True),\n",
    "      StructField(\"country\", StringType(), True),\n",
    "      StructField(\"random_date\", TimestampType(), True)\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa67d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_deserialize_udf = udf(deserialize_avro, returnType=person_avro_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cf8ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_avro_df = df_1.withColumn(\"avro_value\", avro_deserialize_udf(col(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6db19dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+---------+------+--------------------+-------------+--------------------+\n",
      "| key|               value|       topic|partition|offset|           timestamp|timestampType|          avro_value|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+--------------------+\n",
      "|null|[02 0C 43 79 6E 6...|persons-avro|        0|     0|2021-10-09 14:22:...|            0|{1, Cyndie, Chain...|\n",
      "|null|[04 0A 45 64 79 7...|persons-avro|        0|     1|2021-10-09 14:22:...|            0|{2, Edyth, Bultma...|\n",
      "|null|[06 0C 4D 69 67 6...|persons-avro|        0|     2|2021-10-09 14:22:...|            0|{3, Mignon, Martg...|\n",
      "|null|[08 0A 41 6C 6C 6...|persons-avro|        0|     3|2021-10-09 14:22:...|            0|{4, Allis, Aberno...|\n",
      "|null|[0A 12 4D 6F 72 6...|persons-avro|        0|     4|2021-10-09 14:22:...|            0|{5, Morganica, Qu...|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "persons_avro_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a42d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_avro_df_formatted = (\n",
    "   persons_avro_df.select(\n",
    "     col(\"timestamp\").alias(\"event_timestamp\"),\n",
    "     col(\"avro_value.id\").alias(\"id\"),\n",
    "     col(\"avro_value.firstname\").alias(\"first_name\"),\n",
    "     col(\"avro_value.lastname\").alias(\"last_name\"),\n",
    "     col(\"avro_value.email\").alias(\"email\"),\n",
    "     col(\"avro_value.profession\").alias(\"profession\"),\n",
    "     col(\"avro_value.city\").alias(\"city\"),\n",
    "     col(\"avro_value.country\").alias(\"country\"),\n",
    "     col(\"avro_value.random_date\").alias(\"random_date\")  \n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab9725ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----------+-----------+--------------------+-----------+----------+--------------------+-------------------+\n",
      "|     event_timestamp| id|first_name|  last_name|               email| profession|      city|             country|        random_date|\n",
      "+--------------------+---+----------+-----------+--------------------+-----------+----------+--------------------+-------------------+\n",
      "|2021-10-09 14:22:...|  1|    Cyndie|     Chaing|Cyndie.Chaing@yop...|     worker|     Lhasa|              Mexico|1922-03-19 21:00:00|\n",
      "|2021-10-09 14:22:...|  2|     Edyth|    Bultman|Edyth.Bultman@yop...|firefighter|   Cologne|         New Zealand|1907-04-11 21:29:43|\n",
      "|2021-10-09 14:22:...|  3|    Mignon|Martguerita|Mignon.Martguerit...|     worker|Concepción|       Bouvet Island|1957-09-27 21:00:00|\n",
      "|2021-10-09 14:22:...|  4|     Allis|    Abernon|Allis.Abernon@yop...|  developer|   Geelong|         Isle of Man|1968-08-31 21:00:00|\n",
      "|2021-10-09 14:22:...|  5| Morganica|      Quent|Morganica.Quent@y...|     worker|   Pattaya|Saint Pierre and ...|1989-10-25 21:00:00|\n",
      "+--------------------+---+----------+-----------+--------------------+-----------+----------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons_avro_df_formatted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b3f75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_avro_df_counted = (\n",
    "  persons_avro_df_formatted.groupby(col(\"profession\"))\n",
    "                          .agg(count(col(\"profession\")).alias(\"profession_count\")) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62542c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|    profession|profession_count|\n",
      "+--------------+----------------+\n",
      "|     developer|           19878|\n",
      "|   firefighter|           20048|\n",
      "|police officer|           19905|\n",
      "|        worker|           20154|\n",
      "|        doctor|           20015|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:==================================================>     (67 + 1) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "persons_avro_df_counted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f55fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
